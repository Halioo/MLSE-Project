{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compte-rendu - projet de génération d'un modèle de détection de mouvement de balancier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC\n",
    "* [Partie 1](#1)\n",
    "* [Partie 2](#2)\n",
    "* [Partie 3](#3)\n",
    "* [Partie 4](#4)\n",
    "* [Partie 5](#5)\n",
    "* [Partie 6](#6)\n",
    "* [Partie 7](#7)\n",
    "* [Partie 8](#8)\n",
    "* [Partie 9](#9)\n",
    "* [Partie 10](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation <a class=\"anchor\" id=\"1\"></a>\n",
    "Import des packages utile\n",
    "\n",
    "* `pandas` : librairie pour manipuler les données (`DataFrame`)\n",
    "* `numpy` : librairie mathématique\n",
    "* `matplotlib` : librairie graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables liées à l'environnement\n",
    "\n",
    "# seed utilisée pour garantir la reproductibilité des résultats\n",
    "SEED = 42\n",
    "\n",
    "# Fréquence d'échantillonnage de la carte\n",
    "SENSORS_SAMPLING_RATE = 50 # Hz\n",
    "\n",
    "# Chemin d'accès aux données\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions utilitaires\n",
    "\n",
    "def flatten(list):\n",
    "    return [item for sublist in list for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "Nous avons au préalable réalisé des enregistrements de données des capteurs (disponibles dans le dossier `/data`).\n",
    "Nous avons stocké dans un [Google sheet](https://docs.google.com/spreadsheets/d/1By59dQ56zL_kP0tW9Iyf4FppyEvJtcwnuG4gx1iokpM/edit?usp=sharing) si les captures correspondent à un état de balancier, ou non.\n",
    "\n",
    "Nous devons traiter cette donnée brute pour la rendre plus compréhensible et interprétable par Keras.\n",
    "\n",
    "A des fins d'études nous sauvegarderons l'état du dataset dans plusieurs variables:\n",
    "* `raw_dataset` : données brutes\n",
    "* `trimed_dataset` : données après avoir éliminé les limites\n",
    "* `sampled_dataset` : données après avoir été échantillonnées\n",
    "* `bounded_dateset` : données après avoir été réduites à un certain intervalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "dataset = []\n",
    "\n",
    "csvs = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f))]\n",
    "for filename in csvs:\n",
    "    dataset.append(pd.read_csv(DATA_PATH + filename))\n",
    "\n",
    "raw_dataset = dataset.copy()\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim\n",
    "\n",
    "La première étape est de trim les enregistrements pour ne garder que la partie qui nous intéresse.\n",
    "Cela est surtout important dans le cas où notre capture représente un mouvement de balancier, car la donnée brute inclut, au début et à la fin, des instants où l'objet ne se balance pas. Cela correspond au temps qu'il s'écoule entre l'activation de la capture et le début du mouvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limites des données utiles de chaque fichier .csv\n",
    "LIMITES = [ [60, 650],   [50, 350],  [100, 600],\n",
    "            [0, 550],    [0, 550],   [150, 250],\n",
    "            [0, 700],    [75, 820],  [0, 500],\n",
    "            [50, 550],   [50, 750],  [50, 900],\n",
    "            [0, 900],    [50, 1000], [100, 1200],\n",
    "            [50, 1300],  [50, 900],  [200, 700],\n",
    "            [0, 60],     [0, 60],    [0, 25],\n",
    "            [100, 800],  [100, 900], [100, 600],\n",
    "            [100, 850],  [50, 900],  [100, 1500],\n",
    "            [100, 2200], [30, 300],  [0, 650],\n",
    "            [0, 550],    [100, 500], [0, 710]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimDataset(ds):\n",
    "    for i in range(len(ds)):\n",
    "        ds[i] = ds[i][LIMITES[i][0]:LIMITES[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimDataset(dataset)\n",
    "\n",
    "trimed_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset contient maintenant une liste de dataframe de donnes brutes réduites à la partie qui nous intéresse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echantillonnage\n",
    "\n",
    "Dans cette partie nous allons échantillonner les données, c'est à dire que nous allons réduire le nombre de point de données pour un intervalle de temps donné.\n",
    "\n",
    "**Raisonnement**\n",
    "\n",
    "En se basant sur le [théorème de Shannon](), on peut déduire que la fréquence d'échantillonnage nécessaire est deux fois plus grande que la fréquence du signal à détecter.\n",
    "Dans notre cas on a donc simplement besoin d'un échantillonnage de deux fois la fréquence maximale de balancier.\n",
    "De par nos expériences on considère que le balancier maximal possible est de 5 Hz.\n",
    "Notre échantillonnage sera donc de **10 Hz**.\n",
    "\n",
    "**Intérêt**\n",
    "\n",
    "cette méthode a deux intérêts :\n",
    "* Réduire le nombre de points de données, et donc réduire le nombre de neurones d'entrée de notre modèle. Cela permet de réduire le temps de calcul.\n",
    "* Chaque enregistrement contient plus de données que nécessaire, on peut donc les sous-diviser en plusieurs échantillons. Cela permet d'augmenter la data à notre disposition. Ici on multiplie la taille de notre dataset par 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SAMPLING_RATE = 10 # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante `sampleDf` permet de sous-diviser une dataframe en plusieurs sous-échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDf(df, scaleFactor):\n",
    "    res = []\n",
    "    for i in range(scaleFactor):\n",
    "        res.append(df.iloc[lambda x: x.index % scaleFactor == i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalingFactor = SENSORS_SAMPLING_RATE // DATA_SAMPLING_RATE\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = sampleDf(dataset[i], scalingFactor)\n",
    "\n",
    "sampled_dataset = flatten(dataset.copy())\n",
    "len(sampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction de l'intervalle\n",
    "\n",
    "Maintenant que nous avons échantillonné les données, nous allons pouvoir maintenant les sous-diviser en intervalles de temps constants.\n",
    "Ici nous avons choisi de prendre en compte des fenêtre de 2 secondes, soit 20 points de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_TIME = 2 # s\n",
    "\n",
    "WINDOW_LENGTH = WINDOW_TIME * DATA_SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceDf(df, step):\n",
    "    res = []\n",
    "    while (len(df) > step):\n",
    "        res.append(df.iloc[:step])\n",
    "        df = df.iloc[1:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    for ii in range(len(dataset[i])):\n",
    "        dataset[i][ii] = sliceDf(dataset[i][ii], WINDOW_LENGTH)\n",
    "    dataset[i] = flatten(dataset[i])\n",
    "\n",
    "sliced_dataset = flatten(dataset.copy())\n",
    "len(sliced_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite valider les données obtenues en vérifiant que toutes les dataframes obtenues sont bien toutes sur un intervalle de 2 secondes, ie. qu'elles contiennent bien 20 points de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la données\n",
    "# Toutes les dataframes font-elles bien la même taille ?\n",
    "\n",
    "sizes = []\n",
    "for df in sliced_dataset:\n",
    "    if len(df) not in sizes:\n",
    "        sizes.append(len(df))\n",
    "sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean des données inutiles\n",
    "\n",
    "Maintenant on va maintenant faire le choix des colonnes de données inutiles.\n",
    "Pour rappel voilà les colonnes de données disponibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi de conserver toutes les données disponibles dans un premier temps.\n",
    "En effet, la colonne du temps n'as pas de sens car un mouvement de balancier doit pouvoir être détecté indépendamment du moment où il apparaît. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNUSED_DATA_COLUMN = [\"T [ms]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDf(df):\n",
    "    return df.drop(columns=UNUSED_DATA_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][0].columns)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    for ii in range(len(dataset[i])):\n",
    "        dataset[i][ii] = cleanDf(dataset[i][ii])\n",
    "\n",
    "print(dataset[0][0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de cette étape, la variable `dataset` contient une liste de dataframe de données traitées et normalisées. Chacune contient une fenêtre de 2 secondes de captures échantillonnés à 10Hz, correspondant à 20 points * 6 capteurs = 120 points de donnée.\n",
    "\n",
    "Ce sont ces valeurs qui seront passés en entrée de notre modèle pour l'entraîner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labellisation\n",
    "\n",
    "Maintenant que nous avons normalisé notre dataset, nous allons pouvoir maintenant labelliser les données.\n",
    "Ce processus consiste à associer à chaque dataframe une valeur numérique représentant le résultat souhaité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch des résultats de balancier\n",
    "\n",
    "On commence par aller récupérer les résultats attendus des différentes captures via le Google Sheet.\n",
    "Pour cela on a crée un utilitaire `SheetAPI` qui permet de récupérer sous la forme d'un tableau les éléments notés dedans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gSheet import SheetAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ID and range of a sample spreadsheet.\n",
    "SPREADSHEET_ID = '1By59dQ56zL_kP0tW9Iyf4FppyEvJtcwnuG4gx1iokpM'\n",
    "api = SheetAPI(SPREADSHEET_ID)\n",
    "api.connect()\n",
    "\n",
    "Y = api.getValues(\"A2:D100\")\n",
    "Ycolumns = Y[0]\n",
    "Y = Y[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    Y[i] = [Y[i] for _ in dataset[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On valide que les sorties contiennent bien le bon nombre de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_dataset = 0\n",
    "size_Y = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    size_dataset += len(dataset[i])\n",
    "    size_Y += len(Y[i])\n",
    "    assert(len(Y[i]) == len(dataset[i]))\n",
    "\n",
    "print(\"size_dataset: \", size_dataset)\n",
    "print(\"size_Y: \", size_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation de la forme des données\n",
    "\n",
    "Maintenant que les sorties sont correctement créées, on applatit les deux listes, pour que chaque élément corresponde à une fenêtre de 2 secondes de capture, échantillonné à 10Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = flatten(Y)\n",
    "dataset = flatten(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des données de tests\n",
    "\n",
    "Une fois nos données correctement associées à leurs résultats attendus, nous pouvons maintenant créer des données de tests.\n",
    "Pour être plus précis, nous allons sortir un tiers de notre dataset pour ne pas l'utiliser comme données d'entraînement, et l'utiliser pour la validation de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_train, E_test, Y_train, Y_test = train_test_split(dataset, Y, test_size=0.33, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération du modèle\n",
    "\n",
    "Maintenant que nous avons traité nos données brutes, il faut configurer un modèle capable de les interpréter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model  # install graphviz on OS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration des paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # Instanciation du modèle\n",
    "\n",
    "# TODO je suis pas sûr que cette couche soit celle d'entrée, peut-être pas nécessire ?\n",
    "model.add(Dense(120, input_dim=1, activation='sigmoid')) # Ajout de la couche d'entrée\n",
    "model.add(Dense(2, activation='sigmoid')) # Ajout de la couche de sortie\n",
    "\n",
    "# TODO Configuration de l'optimizer\n",
    "opt = Adam()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "history = model.fit(E_train, Y_train, validation_split=0.15, shuffle=False, epochs=400, verbose=0, batch_size=5)\n",
    "\n",
    "print(\"Temps passé : %.2fs\" % (timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données\n",
    "On commence par importer les données depuis un fichier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('data/old/SensorTile_Log_N008.csv')\n",
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataframe(df):\n",
    "    df = df.drop(columns=[\"T [ms]\"])\n",
    "    return df\n",
    "\n",
    "def sliceDf(df, step):\n",
    "    res = []\n",
    "    while (len(df) > step):\n",
    "        res.append(df.iloc[:step])\n",
    "        df = df.iloc[step:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = cleanDataframe(d)\n",
    "d.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = sliceDf(d, 100)\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[\"AccX [mg]\"])\n",
    "plt.plot(d[\"AccY [mg]\"])\n",
    "plt.plot(d[\"AccZ [mg]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[\"GyroX [mdps]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the data\n",
    "\n",
    "Given the data, we can parse it to extract the information we need.\n",
    "\n",
    "First we slice the dataframe into multiple 1-sec **rolling** windows\n",
    "Then we multiply the data by sampling the data into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 50 # Hz\n",
    "DATA_RATE = 20 # Hz\n",
    "WINDOW_LENGTH = 2000 # (in ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliceDf(df, step):\n",
    "    res = []\n",
    "    while (len(df) > step):\n",
    "        res.append(df.iloc[:step])\n",
    "        df = df.iloc[1:]\n",
    "    return res\n",
    "\n",
    "def removeTime(df):\n",
    "    df = df.drop(columns=[\"T [ms]\"])\n",
    "    return df\n",
    "\n",
    "def sampleDf(df, sample):\n",
    "    res = []\n",
    "    for i in range(0, sample): # A tester\n",
    "        res.append(df.iloc[lambda x: x.index % sample == i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('data/balancier0.csv')\n",
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.tail(10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e76f0dc653f8d2ef4c080a707dc31e02e03574b68b8024816724bcd14c82a0bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
